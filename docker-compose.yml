
services:

  server:
    image: python:3.9-slim
    working_dir: /app
    volumes:
      - .:/app
    command: /bin/sh -c "pip install --no-cache-dir -r requirements.txt && python -u server.py"
    depends_on:
      - logstash
    environment:
      - TCP_IP=logstash
      - TCP_PORT=5002

  logstash:
    image: docker.elastic.co/logstash/logstash:7.17.9
    volumes:
      - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    ports:
      - "5002:5002"
    depends_on:
        - zookeeper
        - init-kafka
        - broker

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  broker:
    image: confluentinc/cp-kafka:latest
    hostname: broker
    container_name: broker
    ports:
      - "9092:9092"
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://broker:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1

  init-kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - broker
      - zookeeper
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      # blocks until kafka is reachable
      kafka-topics --bootstrap-server broker:29092 --list

      echo -e 'Creating kafka topics'
      kafka-topics --bootstrap-server broker:29092 --create --if-not-exists --topic data --replication-factor 1 --partitions 1
    
      echo -e 'Successfully created the following topics:'
      kafka-topics --bootstrap-server broker:29092 --list
      "

  spark:
    image: spark_3.4.2
    hostname: spark
    container_name: spark
    volumes:
      - ./streaming/streaming.py:/opt/tap/streaming.py
    command: > 
      /opt/spark/bin/spark-submit --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,org.elasticsearch:elasticsearch-spark-30_2.12:8.13.4  /opt/tap/streaming.py
    depends_on:
      elasticsearch:
        condition: service_started
      init-kafka:
        condition: service_completed_successfully

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.13.4
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    mem_limit: 1 GB
    ports:
    - "9200:9200"
    - "9300:9300"
  
  kibana:
    image: docker.elastic.co/kibana/kibana:8.13.4
    container_name: kibana
    environment:
      - ELASTICSEARCH_HOSTS="http://elasticsearch:9200"
    ports:
      - 5601:5601
    depends_on:
      elasticsearch:
        condition: service_started
  
